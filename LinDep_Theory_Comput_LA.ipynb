{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaPxpIH5S5PCNUPSzob5+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrSubbiah/0.Linear_Algebra/blob/main/LinDep_Theory_Comput_LA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> **Reasoning vs Precision**\n",
        "\n",
        "Human theoretical reasoning may sometimes differ from the algorithmic approaches used by computers. This is neither new nor unusual; rather, it is natural for machines, which are not able (or not designed) to understand symbolic forms and exact real numbers in the way humans do. Therefore, human reasoning and machine floating-point precision need not coincide. For further reading on this topic, see the *Deep Learning* (DL) book by Goodfellow et al.:\n",
        "[https://www.deeplearningbook.org/contents/numerical.html](https://www.deeplearningbook.org/contents/numerical.html)\n",
        "\n",
        "These notes attempt to exemplify one such situation.\n",
        "\n",
        "Let us consider $X_1$ and $Y$, both randomly generated vectors of length $n$. Let $X_2 = 2 \\times X_1$ (or simply $X_2 = 2X_1$), and let $X$ be a matrix whose columns are $X_1$ and $X_2$.\n",
        "\n",
        "Human factual (theoretical) reasoning immediately leads to the following:\n",
        "\n",
        "* $X^TX$ is a singular matrix\n",
        "* At least one eigenvalue must be zero\n",
        "* A warning not to proceed if one attempts to fit a linear model $Y \\sim X_1 + X_2$\n",
        "\n",
        "All of these (and similar other conclusions) are immediate for human reasoning. A human decision might be to drop a variable or revise the model. However, machine floating-point computations behave differently, and their behavior largely depends on how the algorithms are **calibrated**.\n",
        "\n",
        "Below are examples from standard (and widely used) Python libraries to examine this behavior. In this exercise, another observation emerges: differences in results can arise due to ***algorithmic calibration***, which itself can be an important learning point.\n",
        "\n",
        "## <font color=\"darkblue\"> Details of Examples\n",
        "\n",
        "* Five different random generators are used for $X_1$ and $Y$\n",
        "* Two different libraries are used for fitting a linear model\n",
        "* Results are compared\n",
        "* A linear model is then fitted using only one dataset generated from a sampler\n",
        "* Discrete uniform generators are used to retain expected similarity between human reasoning and machine computation, since floating-point issues do not arise when $X_1$ consists of integers\n",
        "\n",
        "The conclusions from the summary tables are self-explanatory.\n",
        "\n",
        "Finally, here is an exact line from the DL book:\n",
        "\n",
        "***“Developers of low-level libraries should keep numerical issues in mind when implementing deep learning algorithms.”***\n",
        "\n"
      ],
      "metadata": {
        "id": "tcboaE6v_gcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> Example 1\n",
        "\n",
        "## <font color=\"blue\"> Necessary Libraries"
      ],
      "metadata": {
        "id": "jxxJn2Jl878K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojvsNfb7nfkp"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 0: Imports\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"blue\"> Fit the model and summarize the comparative results"
      ],
      "metadata": {
        "id": "NuyQE_A09LV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------\n",
        "# Initial set up\n",
        "# ------------------------------------------------------------\n",
        "N = 50\n",
        "SEED = 123\n",
        "\n",
        "results = []\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Fit LM with two libraries\n",
        "# ------------------------------------------------------------\n",
        "def fit_statsmodels(Y, X1, X2):\n",
        "    X = sm.add_constant(np.column_stack([X1, X2]))\n",
        "    m = sm.OLS(Y, X).fit()\n",
        "    return m.params[0], m.params[1], m.params[2]\n",
        "\n",
        "def fit_sklearn(Y, X1, X2):\n",
        "    X = np.column_stack([X1, X2])\n",
        "    m = LinearRegression(fit_intercept=True).fit(X, Y)\n",
        "    return m.intercept_, m.coef_[0], m.coef_[1]\n",
        "\n",
        "def store(gen_lib, fit_lib, params):\n",
        "    results.append({\n",
        "        \"data_gen_lib\": gen_lib,\n",
        "        \"fit_lib\": fit_lib,\n",
        "        \"intercept\": params[0],\n",
        "        \"beta_X1\": params[1],\n",
        "        \"beta_X2\": params[2]\n",
        "    })\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Random number from NUMPY and fit OLS using two libraries\n",
        "# ------------------------------------------------------------\n",
        "np.random.seed(SEED)\n",
        "X1 = np.random.normal(10, 5, N)\n",
        "X2 = 2 * X1\n",
        "Y  = np.random.normal(0, 4, N)\n",
        "\n",
        "store(\"NumPy\", \"statsmodels\", fit_statsmodels(Y, X1, X2))\n",
        "store(\"NumPy\", \"sklearn\",     fit_sklearn(Y, X1, X2))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Random number from SCIPY and fit OLS using two libraries\n",
        "# ------------------------------------------------------------\n",
        "np.random.seed(SEED)\n",
        "X1 = norm.rvs(10, 5, N)\n",
        "X2 = 2 * X1\n",
        "Y  = norm.rvs(0, 4, N)\n",
        "\n",
        "store(\"SciPy\", \"statsmodels\", fit_statsmodels(Y, X1, X2))\n",
        "store(\"SciPy\", \"sklearn\",     fit_sklearn(Y, X1, X2))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Random number from PANDAS and fit OLS using two libraries\n",
        "# ------------------------------------------------------------\n",
        "np.random.seed(SEED)\n",
        "df = pd.DataFrame({\"X1\": np.random.normal(10, 5, N)})\n",
        "df[\"X2\"] = 2 * df[\"X1\"]\n",
        "df[\"Y\"]  = np.random.normal(0, 4, N)\n",
        "\n",
        "store(\"Pandas\", \"statsmodels\", fit_statsmodels(df[\"Y\"], df[\"X1\"], df[\"X2\"]))\n",
        "store(\"Pandas\", \"sklearn\",     fit_sklearn(df[\"Y\"], df[\"X1\"], df[\"X2\"]))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Random number from PYTORCH and fit OLS using two libraries\n",
        "# ------------------------------------------------------------\n",
        "torch.manual_seed(SEED)\n",
        "X1 = torch.normal(10.0, 5.0, size=(N,))\n",
        "X2 = 2 * X1\n",
        "Y  = torch.normal(0.0, 4.0, size=(N,))\n",
        "\n",
        "store(\"PyTorch\", \"statsmodels\", fit_statsmodels(Y.numpy(), X1.numpy(), X2.numpy()))\n",
        "store(\"PyTorch\", \"sklearn\",     fit_sklearn(Y.numpy(), X1.numpy(), X2.numpy()))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Random number from TENSORFLOW and fit OLS using two libraries\n",
        "# ------------------------------------------------------------\n",
        "tf.random.set_seed(SEED)\n",
        "X1 = tf.random.normal((N,), mean=10.0, stddev=5.0)\n",
        "X2 = 2 * X1\n",
        "Y  = tf.random.normal((N,), mean=0.0, stddev=4.0)\n",
        "\n",
        "store(\"TensorFlow\", \"statsmodels\", fit_statsmodels(Y.numpy(), X1.numpy(), X2.numpy()))\n",
        "store(\"TensorFlow\", \"sklearn\",     fit_sklearn(Y.numpy(), X1.numpy(), X2.numpy()))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# FINAL RESULTS TABLE\n",
        "# ------------------------------------------------------------\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUSeKHF2obiY",
        "outputId": "10145906-db55-412f-820d-90e704256117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  data_gen_lib      fit_lib  intercept   beta_X1   beta_X2\n",
            "0        NumPy  statsmodels   0.777252 -0.012188 -0.024377\n",
            "1        NumPy      sklearn   0.777252 -0.012188 -0.024377\n",
            "2        SciPy  statsmodels   0.777252 -0.012188 -0.024377\n",
            "3        SciPy      sklearn   0.777252 -0.012188 -0.024377\n",
            "4       Pandas  statsmodels   0.777252 -0.012188 -0.024377\n",
            "5       Pandas      sklearn   0.777252 -0.012188 -0.024377\n",
            "6      PyTorch  statsmodels  -1.741812  0.021809  0.043617\n",
            "7      PyTorch      sklearn  -1.741812  0.021809  0.043617\n",
            "8   TensorFlow  statsmodels  -0.482770  0.000956  0.001913\n",
            "9   TensorFlow      sklearn  -0.482770  0.000956  0.001913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3909263479.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  return m.params[0], m.params[1], m.params[2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> Example 2\n",
        "\n",
        "## <font color=\"blue\"> ONE DATASET, MULTIPLE LM FITS"
      ],
      "metadata": {
        "id": "KzH_NWY79Sq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------------------------------------\n",
        "N = 50\n",
        "SEED = 123\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DATA GENERATION (single generator: NumPy)\n",
        "# ------------------------------------------------------------\n",
        "np.random.seed(SEED)\n",
        "X1 = np.random.normal(10, 5, N)\n",
        "X2 = 2 * X1\n",
        "Y  = np.random.normal(0, 4, N)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# FIT: STATSMODELS\n",
        "# ------------------------------------------------------------\n",
        "X_sm = sm.add_constant(np.column_stack([X1, X2]))\n",
        "sm_model = sm.OLS(Y, X_sm).fit()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# FIT: SKLEARN\n",
        "# ------------------------------------------------------------\n",
        "X_sk = np.column_stack([X1, X2])\n",
        "sk_model = LinearRegression(fit_intercept=True).fit(X_sk, Y)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONSOLIDATED RESULTS DF\n",
        "# ------------------------------------------------------------\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        \"data_gen_lib\": \"NumPy\",\n",
        "        \"fit_lib\": \"statsmodels\",\n",
        "        \"intercept\": sm_model.params[0],\n",
        "        \"beta_X1\": sm_model.params[1],\n",
        "        \"beta_X2\": sm_model.params[2]\n",
        "    },\n",
        "    {\n",
        "        \"data_gen_lib\": \"NumPy\",\n",
        "        \"fit_lib\": \"sklearn\",\n",
        "        \"intercept\": sk_model.intercept_,\n",
        "        \"beta_X1\": sk_model.coef_[0],\n",
        "        \"beta_X2\": sk_model.coef_[1]\n",
        "    }\n",
        "])\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuJzzcKoq5FG",
        "outputId": "8c52ea1e-4bba-4703-801b-4efa9b90b806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  data_gen_lib      fit_lib  intercept   beta_X1   beta_X2\n",
            "0        NumPy  statsmodels   0.777252 -0.012188 -0.024377\n",
            "1        NumPy      sklearn   0.777252 -0.012188 -0.024377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> Example 3\n",
        "\n",
        "## <font color=\"blue\">  DISCRETE X1, SAME DATA, MULTIPLE LM FITS"
      ],
      "metadata": {
        "id": "Sg82e5FD9lsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------------------------------------\n",
        "N = 50\n",
        "SEED = 123\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DATA GENERATION (Discrete Uniform)\n",
        "# ------------------------------------------------------------\n",
        "np.random.seed(SEED)\n",
        "\n",
        "X1 = np.random.randint(1, 101, size=N)   # discrete uniform {1,...,100}\n",
        "X2 = 2 * X1\n",
        "Y  = np.random.normal(0, 4, size=N)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# FIT: STATSMODELS\n",
        "# ------------------------------------------------------------\n",
        "X_sm = sm.add_constant(np.column_stack([X1, X2]))\n",
        "sm_model = sm.OLS(Y, X_sm).fit()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# FIT: SKLEARN\n",
        "# ------------------------------------------------------------\n",
        "X_sk = np.column_stack([X1, X2])\n",
        "sk_model = LinearRegression(fit_intercept=True).fit(X_sk, Y)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONSOLIDATED RESULTS DF\n",
        "# ------------------------------------------------------------\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        \"data_gen_lib\": \"NumPy (discrete uniform)\",\n",
        "        \"fit_lib\": \"statsmodels\",\n",
        "        \"intercept\": sm_model.params[0],\n",
        "        \"beta_X1\": sm_model.params[1],\n",
        "        \"beta_X2\": sm_model.params[2]\n",
        "    },\n",
        "    {\n",
        "        \"data_gen_lib\": \"NumPy (discrete uniform)\",\n",
        "        \"fit_lib\": \"sklearn\",\n",
        "        \"intercept\": sk_model.intercept_,\n",
        "        \"beta_X1\": sk_model.coef_[0],\n",
        "        \"beta_X2\": sk_model.coef_[1]\n",
        "    }\n",
        "])\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r-gQXJtro2j",
        "outputId": "1bf7f036-ca8d-4d48-d4f1-caeaa9098281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               data_gen_lib      fit_lib  intercept   beta_X1   beta_X2\n",
            "0  NumPy (discrete uniform)  statsmodels   1.088742 -0.002019 -0.004037\n",
            "1  NumPy (discrete uniform)      sklearn   1.088742 -0.002019 -0.004037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> Example 4\n",
        "\n",
        "## <font color=\"blue\">  Misc - Matrix Computations"
      ],
      "metadata": {
        "id": "anizWFCq9w3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------------------------------------\n",
        "N = 50\n",
        "SEED = 123\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DATA (EXACT LD)\n",
        "# ------------------------------------------------------------\n",
        "X1 = np.random.normal(10, 5, N)\n",
        "X2 = 2 * X1\n",
        "\n",
        "X = np.column_stack([X1, X2])\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# COMPUTE X'X\n",
        "# ------------------------------------------------------------\n",
        "XTX = X.T @ X\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DETERMINANT AND EIGENVALUES\n",
        "# ------------------------------------------------------------\n",
        "det_XTX = np.linalg.det(XTX)\n",
        "eigvals_XTX = np.linalg.eigvals(XTX)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# OUTPUT\n",
        "# ------------------------------------------------------------\n",
        "print(\"X'X matrix:\\n\", XTX)\n",
        "print(\"\\ndet(X'X):\", det_XTX)\n",
        "print(\"\\nEigenvalues of X'X:\", eigvals_XTX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlrzFkYCL5BN",
        "outputId": "929b74ed-0abf-492d-ade0-6880b885d34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X'X matrix:\n",
            " [[ 6835.58041892 13671.16083785]\n",
            " [13671.16083785 27342.3216757 ]]\n",
            "\n",
            "det(X'X): 0.0\n",
            "\n",
            "Eigenvalues of X'X: [-3.63797881e-12  3.41779021e+04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"maroon\"> **Notes to Leaders in Data Science / Machine Learning**\n",
        "\n",
        "Leaders themselves don’t code in C or assembly, they should care about low-level calibration because:\n",
        "\n",
        "- Numerical stability matters: small floating-point differences can cause models to fail, produce NaNs, or give inconsistent results across platforms.\n",
        "\n",
        "- Cross-library consistency: different low-level implementations (BLAS, LAPACK, MKL, GPU kernels) can produce slightly different outputs.\n",
        "\n",
        "- Edge cases and safety: in production, edge cases may break high-level assumptions.\n",
        "\n",
        "- Efficient resource usage: low-level optimization affects speed and memory.\n",
        "\n",
        "So a good leader in DS/ML should:\n",
        "\n",
        "- Ensure the team understands low-level calibration and precision limits.\n",
        "\n",
        "- Encourage testing across platforms and libraries.\n",
        "\n",
        "- Don’t blindly trust high-level outputs, especially in critical applications\n",
        "\n",
        "---\n",
        "\n",
        "1. **Understand numerical foundations**\n",
        "\n",
        "   * Know that high-level frameworks rely on low-level libraries (C/Fortran/GPU kernels) and floating-point arithmetic.\n",
        "   * Encourage awareness of **precision, rounding errors, and numerical stability**.\n",
        "\n",
        "2. **Promote cross-platform testing**\n",
        "\n",
        "   * Ensure models and pipelines are tested across libraries, platforms, and devices to detect subtle inconsistencies.\n",
        "\n",
        "3. **Encourage edge-case exploration**\n",
        "\n",
        "   * Teams should identify situations where floating-point limits or ill-conditioned matrices can break assumptions.\n",
        "\n",
        "4. **Foster communication between levels**\n",
        "\n",
        "   * High-level developers and low-level engineers should collaborate: optimizations, calibration, and fixes in low-level code must inform high-level design.\n",
        "\n",
        "5. **Prioritize reproducibility and auditability**\n",
        "\n",
        "   * Document random seeds, library versions, and computation details to track where differences arise.\n",
        "\n",
        "6. **Balance trust and verification**\n",
        "\n",
        "   * Don’t blindly trust high-level outputs — cultivate a culture of **reasoning over precision**, i.e., **understand why a result occurs, not just that it occurs**."
      ],
      "metadata": {
        "id": "uXg2sZ2tBh8J"
      }
    }
  ]
}