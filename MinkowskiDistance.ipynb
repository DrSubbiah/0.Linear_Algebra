{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm2j9WV5YiSzfST+tsQzPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrSubbiah/0.Linear_Algebra/blob/main/MinkowskiDistance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minkowski Distance — Notes\n",
        "## Historical Notes: Hermann Minkowski and the Minkowski Distance\n",
        "\n",
        "**Hermann Minkowski (1864–1909)** was a German mathematician of Baltic-German origin, best known for his work in geometry, number theory, and mathematical physics.\n",
        "\n",
        "### The Person\n",
        "- Minkowski studied mathematics in Germany and later became a professor at several universities, including Göttingen.\n",
        "- He was a teacher of **Albert Einstein** and played a crucial role in formalizing the mathematical structure of Einstein’s theory of relativity.\n",
        "- His most famous contribution is **Minkowski spacetime (1908)**, which unified space and time into a four-dimensional geometric framework.\n",
        "\n",
        "### Development of the Minkowski Distance\n",
        "- The **Minkowski distance** arises from Minkowski’s work on **norms and geometry in $ \\mathbb{R}^n $**, particularly in the context of **convex geometry**.\n",
        "- It generalizes earlier distance measures:\n",
        "  - **Manhattan distance** (linked to lattice geometry)\n",
        "  - **Euclidean distance** (classical geometry)\n",
        "- By introducing a parameter $ p $, Minkowski provided a unified mathematical framework that includes multiple distance metrics as special cases.\n",
        "\n",
        "### Impact and Legacy\n",
        "- The Minkowski distance became foundational in:\n",
        "  - Functional analysis (via $ L^p $ norms)\n",
        "  - Geometry and optimization\n",
        "  - Modern data science and machine learning\n",
        "- His ideas influenced later developments in normed vector spaces and metric spaces.\n",
        "\n",
        "**In short:**  \n",
        "Minkowski’s work transformed distance from a single geometric concept into a flexible, parameterized family of metrics, with lasting influence across mathematics, physics, and computer science.\n",
        "\n",
        "## General Definition\n",
        "The **Minkowski distance** of order $ p $ between two points  \n",
        "$$\n",
        "\\mathbf{X} = (x_1, x_2, \\dots, x_n), \\quad\n",
        "\\mathbf{Y} = (y_1, y_2, \\dots, y_n)\n",
        "$$\n",
        "in $ \\mathbb{R}^n $ is defined as:\n",
        "\n",
        "$$\n",
        "d_p(\\mathbf{X}, \\mathbf{Y}) =\n",
        "\\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p},\n",
        "\\quad p \\ge 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Special Cases\n",
        "\n",
        "### 1. Manhattan Distance ( $ p = 1 $ )\n",
        "Also known as **L1 distance** or **Taxicab distance**.\n",
        "\n",
        "$$\n",
        "d_1(\\mathbf{X}, \\mathbf{Y}) =\n",
        "\\sum_{i=1}^{n} |x_i - y_i|\n",
        "$$\n",
        "\n",
        "**Interpretation:**  \n",
        "Distance measured by moving only along coordinate axes (like city blocks).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Euclidean Distance ( $ p = 2 $ )\n",
        "Also known as **L2 distance**.\n",
        "\n",
        "$$\n",
        "d_2(\\mathbf{X}, \\mathbf{Y}) =\n",
        "\\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
        "$$\n",
        "\n",
        "**Interpretation:**  \n",
        "Straight-line (ordinary geometric) distance.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Chebyshev Distance ( $ p \\to \\infty $ )\n",
        "Also known as **L∞ distance** or **Maximum distance**.\n",
        "\n",
        "$$\n",
        "d_\\infty(\\mathbf{X}, \\mathbf{Y}) =\n",
        "\\lim_{p \\to \\infty} d_p(\\mathbf{X}, \\mathbf{Y})\n",
        "= \\max_{i} |x_i - y_i|\n",
        "$$\n",
        "\n",
        "**Interpretation:**  \n",
        "Distance is determined by the largest difference along any single dimension.\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "- Minkowski distance is a **metric** for $ p \\ge 1 $.\n",
        "- As $ p $ increases, the distance becomes more sensitive to large coordinate differences.\n",
        "- Commonly used in machine learning (e.g., k-NN, clustering).\n",
        "\n",
        "| Distance       | What It Measures     | Think Of It As          | Best When                       |\n",
        "| -------------- | -------------------- | ----------------------- | ------------------------------- |\n",
        "| Manhattan (L1) | Total deviation      | Summing all differences | Many small, independent changes |\n",
        "| Euclidean (L2) | Geometric separation | Straight-line distance  | Smooth, spatial similarity      |\n",
        "| Chebyshev (L∞) | Maximum deviation    | Worst-case difference   | Limits, constraints, tolerances |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Applications of Common Distance Metrics (ML / AI / DS / Statistics)\n",
        "\n",
        "| Distance Metric    | ML / AI Use Cases                                                   | Data Science & Statistics                             | Mathematical Role                              | Why It’s Chosen                                              |\n",
        "| ------------------ | ------------------------------------------------------------------- | ----------------------------------------------------- | ---------------------------------------------- | ------------------------------------------------------------ |\n",
        "| **Manhattan (L1)** | k-NN with sparse data, L1 regularization (LASSO), feature selection | Robust clustering, high-dimensional data analysis     | Induces sparsity, convex norm                  | Penalizes total deviation evenly; less sensitive to outliers |\n",
        "| **Euclidean (L2)** | k-means clustering, PCA, neural networks, SVMs                      | Classical statistical distance, variance-based models | Inner-product norm, geometry of $\\mathbb{R}^n$ | Smooth, rotation-invariant, emphasizes large differences     |\n",
        "| **Chebyshev (L∞)** | Adversarial robustness, constraint-based ML                         | Tolerance bounds, worst-case analysis                 | Uniform norm, max-error control                | Focuses on maximum deviation; ideal for safety & guarantees  |\n",
        "\n",
        "---\n",
        "\n",
        "## Practical Interpretation by Field\n",
        "\n",
        "| Field                | L1 (Manhattan)                   | L2 (Euclidean)                               | L∞ (Chebyshev)                 |\n",
        "| -------------------- | -------------------------------- | -------------------------------------------- | ------------------------------ |\n",
        "| **Machine Learning** | Sparse models, robust similarity | Smooth optimization, geometry-based learning | Adversarial bounds, robustness |\n",
        "| **Data Science**     | Feature-wise differences         | Global similarity                            | Maximum error monitoring       |\n",
        "| **Statistics**       | Median-based methods             | Mean & variance-based methods                | Confidence & tolerance limits  |\n",
        "| **Mathematics**      | Convex optimization              | Hilbert spaces                               | Uniform convergence            |\n",
        "\n",
        "---\n",
        "\n",
        "## Rule-of-Thumb Selection Guide\n",
        "\n",
        "* Use **L1** when:\n",
        "\n",
        "  * Data is high-dimensional or sparse\n",
        "  * You want interpretability or feature selection\n",
        "\n",
        "* Use **L2** when:\n",
        "\n",
        "  * Geometry matters\n",
        "  * You want smooth, differentiable objectives\n",
        "\n",
        "* Use **L∞** when:\n",
        "\n",
        "  * Worst-case error matters\n",
        "  * You need hard guarantees or bounds\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCfhQJq9ZoZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example:\n",
        "\n",
        "## Comparing Distances Between Two Points\n",
        "\n",
        "### Scenario: Let us consider coordinate as a feature (e.g., age, income, score, pixel value)."
      ],
      "metadata": {
        "id": "HFgmTG7cd_L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def Mink_distance(x, y):\n",
        "    \"\"\"\n",
        "    Compute common Minkowski distance special cases between two vectors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : array-like\n",
        "        Input feature vectors of equal length.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary containing L1, L2, and L∞ distances.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x)\n",
        "    y = np.asarray(y)\n",
        "\n",
        "    if x.shape != y.shape:\n",
        "        raise ValueError(\"Input vectors must have the same shape\")\n",
        "\n",
        "    diff = np.abs(x - y)\n",
        "\n",
        "    return {\n",
        "        \"L1 (Manhattan)\": np.sum(diff),\n",
        "        \"L2 (Euclidean)\": np.sqrt(np.sum(diff**2)),\n",
        "        \"L∞ (Chebyshev)\": np.max(diff)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "G0wqsDVseQBQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Usage"
      ],
      "metadata": {
        "id": "SAAhajVIecov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [2, 3, 5]\n",
        "y = [7, 1, 9]\n",
        "\n",
        "distances = Mink_distance(x, y)\n",
        "\n",
        "for name, value in distances.items():\n",
        "    print(f\"{name:18}: {value:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYHc3rQYeav4",
        "outputId": "02bd6adc-7f4e-4ee8-ca6f-305fa336b2e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 (Manhattan)    : 11.000\n",
            "L2 (Euclidean)    : 6.708\n",
            "L∞ (Chebyshev)    : 5.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Accuracy Measured as Distance\n",
        "\n",
        "In supervised learning, predictions and true values live in the same output space.\n",
        "The distance between them does not represent the prediction itself, but rather the lack of accuracy.\n",
        "\n",
        "Smaller distance ⇒ higher prediction accuracy\n",
        "Larger distance ⇒ lower prediction accuracy\n",
        "\n",
        "## Different distance measures quantify prediction accuracy in different ways:\n",
        "\n",
        "- L1 (Manhattan distance) measures total absolute error → overall accuracy\n",
        "\n",
        "- L2 (Euclidean distance) measures error magnitude → penalizes large mistakes\n",
        "\n",
        "- L∞ (Chebyshev distance) measures worst-case error → guaranteed accuracy bounds\n",
        "\n",
        "Thus, distance functions act as accuracy (or error) metrics."
      ],
      "metadata": {
        "id": "Jt9gGKswf55r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Measuring Prediction Accuracy Using Distance\n",
        "\n",
        "#import numpy as np\n",
        "\n",
        "# True values and model predictions\n",
        "true = np.array([100, 80, 60])\n",
        "pred = np.array([95, 82, 70])\n",
        "\n",
        "# Measure prediction accuracy as distance\n",
        "distances =Mink_distance(true, pred)\n",
        "\n",
        "for name, value in distances.items():\n",
        "    print(f\"{name:18}: {value:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt6548prgETw",
        "outputId": "54192b55-6b7b-4aeb-a886-b197eff7b654"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 (Manhattan)    : 17.000\n",
            "L2 (Euclidean)    : 11.358\n",
            "L∞ (Chebyshev)    : 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correct Way to Compare Algorithms (Decision Logic)\n",
        "\n",
        "Step 1: Decide what “accuracy” means\n",
        "\n",
        "Ask before comparing numbers:\n",
        "\n",
        "- Is average error important? → L1\n",
        "\n",
        "  - Lower L1 → better overall accuracy\n",
        "\n",
        "  - Treats all errors equally\n",
        "\n",
        "  - Robust to outliers\n",
        "\n",
        "- Are large mistakes costly? → L2 / RMSE\n",
        "\n",
        "  - Squares errors → large errors dominate\n",
        "\n",
        "  - Sensitive to outliers\n",
        "\n",
        "- Do I need hard guarantees? → L∞\n",
        "\n",
        "  - Only the largest error matters\n",
        "\n",
        "  - Common in safety-critical systems\n",
        "\n",
        "# Practical Rule-of-Thumb\n",
        "\n",
        "| Application                         | Preferred Metric |\n",
        "| ------------------------------------| ---------------- |\n",
        "| General regression                  | MAE (L1) + RMSE  |\n",
        "| Finance / risk                      | RMSE + Max Error |\n",
        "| Safety / control                    | L∞               |\n",
        "| High-dimensional ML                 | MAE              |\n"
      ],
      "metadata": {
        "id": "F5UPIcF-hPFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The connection between **Minkowski distance** and the **bias–variance tradeoff**\n",
        "\n",
        "\n",
        "## Recall: **Bias–Variance Tradeoff and its impact in ML**\n",
        "\n",
        "For a predictive model:\n",
        "\n",
        "$$\n",
        "\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
        "$$\n",
        "\n",
        "* **Bias**: Error from oversimplifying the model (underfitting)\n",
        "* **Variance**: Error from overfitting, highly sensitive to training data\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Connecting Minkowski Distance to Bias–Variance**\n",
        "\n",
        "In **k-NN** or other distance-based algorithms, the choice of **(p) in Minkowski distance** affects bias and variance:\n",
        "\n",
        "#### **a) Small (p) (e.g., (p=1))**\n",
        "\n",
        "* Distance metric emphasizes all dimensions **equally**.\n",
        "* Can lead to more **local neighborhoods**, focusing on closer points in all axes.\n",
        "* If the neighborhood is small or high-dimensional, the model may be **more sensitive to noise**, **higher variance**, lower bias.\n",
        "\n",
        "#### **b) Large (p) (e.g., $p \\to \\infty$)**\n",
        "\n",
        "* Distance is dominated by the largest coordinate difference.\n",
        "* Neighborhoods become less sensitive to smaller differences.\n",
        "* Model becomes **smoother**, less sensitive to training data → **lower variance**, higher bias.\n",
        "\n",
        "#### **c) Intermediate (p)**\n",
        "\n",
        "* Tradeoff between sensitivity to small differences and ignoring outliers.\n",
        "* Can tune (p) to balance **bias and variance** optimally.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Quick Facts**\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "* **Low (p)** → “Pay attention to all tiny differences” → model reacts strongly to noise → **high variance, low bias**\n",
        "\n",
        "* **High (p)** → “Only care about the largest difference” → model smooths over local variations → **low variance, high bias**\n",
        "\n",
        "So in a sense, **Minkowski distance is a hyperparameter that directly affects the bias–variance tradeoff** in distance-based models.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Summary**\n",
        "\n",
        "* Minkowski distance defines how we measure similarity.\n",
        "* The choice of (p) changes the model’s sensitivity:\n",
        "\n",
        "  * Small (p) → high variance, low bias\n",
        "  * Large (p) → low variance, high bias\n",
        "* Thus, tuning (p) is part of controlling the **bias–variance tradeoff** in distance-based learning.s\n"
      ],
      "metadata": {
        "id": "gokqrxJoilHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geometry of the **Neighbourhood**\n",
        "\n",
        "\n",
        "# Consider Set  ${x : d(x, x_0) \\le r}$\n",
        "\n",
        "## 1. What does “distance ≤ r” mean geometrically?\n",
        "\n",
        "Given:\n",
        "\n",
        "* A point $x_0 \\in \\mathbb{R}^n$\n",
        "* A distance function $d(\\cdot,\\cdot)$\n",
        "* A radius $r > 0$\n",
        "\n",
        "The set\n",
        "$$\n",
        "\\boxed{{x : d(x, x_0) \\le r}}\n",
        "$$\n",
        "is called the **ball of radius $r$** centered at $x_0$.\n",
        "\n",
        "Its shape depends **entirely on the distance definition**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Minkowski (Lp) distances\n",
        "\n",
        "For $p \\ge 1$, the Minkowski distance is\n",
        "$$\n",
        "|x - x_0|*p = \\left( \\sum*{i=1}^n |x_i - x_{0,i}|^p \\right)^{1/p}\n",
        "$$\n",
        "\n",
        "The **ball of radius $r$** is\n",
        "$$\n",
        "\\boxed{\n",
        "\\sum_{i=1}^n |x_i - x_{0,i}|^p \\le r^p\n",
        "}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Special cases in 2D\n",
        "\n",
        "Let $x=(x,y), x_0=(x_0,y_0)$\n",
        "\n",
        "\n",
        "### (a) **L2 (Euclidean) distance**\n",
        "\n",
        "$$\n",
        "\\sqrt{(x-x_0)^2 + (y-y_0)^2} \\le r\n",
        "$$\n",
        "\n",
        "Equivalent equation:\n",
        "$$\n",
        "\\boxed{(x-x_0)^2 + (y-y_0)^2 \\le r^2}\n",
        "$$\n",
        "\n",
        "**Geometry:**\n",
        "\n",
        "* Circle (2D)\n",
        "* Sphere (3D)\n",
        "* Rotationally symmetric\n",
        "\n",
        "**Key property:**\n",
        "All directions are treated equally.\n",
        "\n",
        "---\n",
        "\n",
        "### (b) **L1 (Manhattan) distance**\n",
        "\n",
        "$$\n",
        "|x-x_0| + |y-y_0| \\le r\n",
        "$$\n",
        "\n",
        "**Geometry:**\n",
        "\n",
        "* Diamond (a square rotated by 45°)\n",
        "* Edges are straight lines\n",
        "\n",
        "**Why this shape?**\n",
        "The constraint is linear in (|x|) and (|y|), producing flat faces.\n",
        "\n",
        "---\n",
        "\n",
        "### (c) **L∞ (Chebyshev) distance**\n",
        "\n",
        "$$\n",
        "\\max(|x-x_0|, |y-y_0|) \\le r\n",
        "$$\n",
        "\n",
        "Equivalent to:\n",
        "$$\n",
        "\\boxed{\n",
        "|x-x_0| \\le r \\quad \\text{and} \\quad |y-y_0| \\le r\n",
        "}\n",
        "$$\n",
        "\n",
        "**Geometry:**\n",
        "\n",
        "* Axis-aligned square (2D)\n",
        "* Cube (3D)\n",
        "* Hypercube (nD)\n",
        "\n",
        "**Key idea:**\n",
        "Distance is governed by the **worst coordinate deviation**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why these shapes are unavoidable\n",
        "\n",
        "| Norm | Constraint            | Shape   |\n",
        "| ---- | --------------------- | ------- |\n",
        "| L1   | Sum of deviations ≤ r | Diamond |\n",
        "| L2   | Sum of squares ≤ r²   | Circle  |\n",
        "| L∞   | Max deviation ≤ r     | Square  |\n",
        "\n",
        "Each shape is the **solution set of its defining inequality**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Translation invariance\n",
        "\n",
        "All norm balls are translations of the **unit ball**:\n",
        "$$\n",
        "|x - x_0|_p \\le r\n",
        "\\quad \\Longleftrightarrow \\quad\n",
        "\\left|\\frac{x-x_0}{r}\\right|_p \\le 1\n",
        "$$\n",
        "\n",
        "So every ball is:\n",
        "\n",
        "* A scaled version of the unit ball\n",
        "* Shifted to center (x_0)\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Convexity (important for ML)\n",
        "\n",
        "For all (p \\ge 1):\n",
        "\n",
        "$$\n",
        "{x : |x-x_0|_p \\le r} \\text{ is convex}\n",
        "$$\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* Any line segment between two points in the ball stays inside the ball\n",
        "* Guarantees well-behaved optimization and neighborhoods\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Notion of a ball\n",
        "\n",
        "> **A distance ball is the set of all points satisfying the inequality that defines the norm.**\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Connection to ML (contextual insight)\n",
        "\n",
        "* k-NN **neighborhoods** depend on these balls\n",
        "* Decision boundaries change shape with the norm\n",
        "* Bias–variance tradeoff is influenced by neighborhood geometry\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZN0rsLZNNfUa"
      }
    }
  ]
}